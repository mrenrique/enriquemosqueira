{
  
    
        "post0": {
            "title": "Scraping an article & visualizing words in an image's shape",
            "content": "TL;DR &#129299; . Have you ever crossed some blog post, video or presentation having A fun way to show &amp; analize which are the most relevant topics(repeated words) on a text. Bellow I show you some examples whe can create, it&#39;s like art made out of words üé® . . Installing Libraries &#10004;&#65039; . First, you need to install all libraries you&#39;ll be using. . !pip install numpy !pip install matplotlib !pip install newspaper3k !pip install pillow !pip install wordcloud !pip install nltk print(&#39;Library installation Done!&#39;) . Collecting newspaper3k Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211 kB 7.6 MB/s Requirement already satisfied: lxml&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6) Requirement already satisfied: Pillow&gt;=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2) Collecting feedparser&gt;=5.2.1 Downloading feedparser-6.0.8-py3-none-any.whl (81 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81 kB 10.0 MB/s Requirement already satisfied: python-dateutil&gt;=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2) Requirement already satisfied: requests&gt;=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0) Collecting jieba3k&gt;=0.35.1 Downloading jieba3k-0.35.1.zip (7.4 MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.4 MB 60.7 MB/s Requirement already satisfied: nltk&gt;=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5) Collecting feedfinder2&gt;=0.0.4 Downloading feedfinder2-0.0.4.tar.gz (3.3 kB) Collecting cssselect&gt;=0.9.2 Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB) Requirement already satisfied: beautifulsoup4&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3) Collecting tinysegmenter==0.3 Downloading tinysegmenter-0.3.tar.gz (16 kB) Requirement already satisfied: PyYAML&gt;=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13) Collecting tldextract&gt;=2.0.1 Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87 kB 5.8 MB/s Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2&gt;=0.0.4-&gt;newspaper3k) (1.15.0) Collecting sgmllib3k Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2021.10.8) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2.10) Collecting requests-file&gt;=1.4 Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB) Requirement already satisfied: filelock&gt;=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract&gt;=2.0.1-&gt;newspaper3k) (3.4.2) Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k Building wheel for tinysegmenter (setup.py) ... done Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=73e9156b3836ce85dc512439196c449be3d5d33749708372688c3f5d99f1a059 Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa Building wheel for feedfinder2 (setup.py) ... done Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=fe3990b9a89a29307f18a4767d156083b809112c9ca71e5ef2bbb556ddbd874e Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346 Building wheel for jieba3k (setup.py) ... done Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=86ab131c0f61eeaef932b8967c1915eaaabc4a1089ea070652c21631f839e9b4 Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3 Building wheel for sgmllib3k (setup.py) ... done Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=9ad6b6bbe0b981e0262c6294017917f7b9cb32e8af7463272df81aaf6f7f2001 Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k . . Importing Libraries &#129520; . from newspaper import Article from PIL import Image import numpy as np import matplotlib.pyplot as plt from wordcloud import WordCloud, ImageColorGenerator#, STOPWORDS import nltk from nltk.corpus import stopwords nltk.download(&quot;stopwords&quot;) . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Package stopwords is already up-to-date! . True . Step 01: Getting the Corpus from an Article &#128221; . article = Article(&#39;https://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml&#39;) # article = Article(&#39;https://andina.pe/Agencia/noticia-lea-aqui-mensaje-a-nacion-del-presidente-pedro-castillo-879806.aspx&#39;) article.download() article.parse() . Generate a Simple Word Cloud Image . wc = WordCloud() # wc = WordCloud(stopwords=STOPWORDS) wc.generate(article.text) # Display the generated image: plt.imshow(wc, interpolation=&quot;bilinear&quot;) plt.axis(&#39;off&#39;) plt.show() . Generate a Customized Word Cloud Image . stopwords = set(stopwords.words(&#39;spanish&#39;, &#39;english&#39;)) stopwords.update([&#39;ello&#39;, &#39;cinco&#39;, &#39;d√≠a&#39;]) . mask = np.array(Image.open(&#39;repsol.jpg&#39;)) # mask = np.array(Image.open(&#39;pedro-castillo.jpg&#39;)) wc = WordCloud(stopwords=stopwords, background_color=&quot;white&quot;, max_words=2000, mask=mask, ) wc.generate(article.text) # Display the generated image: plt.imshow(wc, interpolation=&quot;bilinear&quot;) plt.axis(&#39;off&#39;) plt.show() . mask = np.array(Image.open(&#39;repsol.jpg&#39;)) # mask = np.array(Image.open(&#39;pedro-castillo.jpg&#39;)) wc = WordCloud(stopwords=stopwords, background_color=&quot;white&quot;, max_words=2000, mask=mask, max_font_size=256, # random_state=42, # width=mask.shape[1], # height=mask.shape[0] ) wc.generate(article.text) # create coloring from image image_colors = ImageColorGenerator(mask) plt.figure(figsize=[16,14]) plt.imshow(wc.recolor(color_func=image_colors), interpolation=&quot;bilinear&quot;) plt.axis(&quot;off&quot;) plt.show() . References: . https://medium.com/towards-data-science/create-word-cloud-into-any-shape-you-want-using-python-d0b88834bc32 | https://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml | .",
            "url": "https://mrenrique.github.io/portfolio/python/matplotlib/numpy/data%20analysis/nlp/text%20analytics/2022/02/08/_02_08_Making_a_Word_Cloud_by_Scraping_an_article.html",
            "relUrl": "/python/matplotlib/numpy/data%20analysis/nlp/text%20analytics/2022/02/08/_02_08_Making_a_Word_Cloud_by_Scraping_an_article.html",
            "date": " ‚Ä¢ Feb 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Title of a Fastpages Notebook Blog Post",
            "content": "What is Data Analysis . A process of inspecting, cleansing, transforming and modeling data with the goal of discovering useful information, informing conclusion and supporting decision-making. Source: Wikipedia . Uses of EDA: . To know the structure and distribution of data | To find relationship between Features | To find relationship between Features and the Target Variable | To find errors, anomalies, outliers | To refine Hipothesis or generate new questions on dataset | . Data Analysis Tools . Programming Languages: Open Source, Free, Extremely Powerful, Steep learning curve . Python | R | Julia | . Auto-managed closed tools: Closed Source, Expensive, Limited, Easy to learn . Power BI | Tableau | Qlik | . The Data Analysis Process . Data Extraction . SQL | Scrapping | File Formats CSV | JSON | XML | . | Consulting APIs | Buying Data | Distributed Databases | . Data Cleaning . Missing values and empty data | Data imputation | Incorrect types | Incorrect or invalid values | Outliers and non relevant data | Statistical sanitization | . Data Wrangling . Hierarchical Data | Handling categorical data | Reshaping and transforming structures | Indexing data for quick access | Merging, combining and joining data | . Analysis . Exploration | Building statistical models | Visualization and representations | Correlation vs Causation analysis | Hypothesis testing | Statistical analysis | Reporting | . Action . Building Machine Learning Models | Feature Engineering | Moving ML into production | Building ETL pipelines | Live dashboard and reporting | Decision making and real-life tests | . https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html . Proceso de organizar, resumir y visualizar un conjunto de datos para extraer informaci√≥n que aporte al logro de objetivos . why using Python and Pandas? . The Pandas library is the key library for Data Science and Analytics and a good place to start for beginners. Often called the &quot;Excel &amp; SQL of Python, on steroids&quot; because of the powerful tools Pandas gives you for editing two-dimensional data tables in Python and manipulating large datasets with ease. . Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with Matplotlib and Seaborn, Pandas provides a wide range of opportunities for visual analysis of tabular data. . The main data structures in Pandas are implemented with Series and DataFrame classes. DataFrames are great for representing real data: rows correspond to instances (examples, observations, etc.), and columns correspond to features of these instances. . Main Keywords . Dataframe: is a main Object in Pandas, It&#39;s used to represent data in rows and columns (Tabular Data) | Pandas: This library needs no introduction as it became the de facto tool for Data Analysis in Python. The name pandas is derived from the term ‚Äúpanel data‚Äù, an econometrics term for datasets that include observations over multiple time periods for the same individuals. | .",
            "url": "https://mrenrique.github.io/portfolio/jupyter/2021/10/03/My-First-Post-By-Colab.html",
            "relUrl": "/jupyter/2021/10/03/My-First-Post-By-Colab.html",
            "date": " ‚Ä¢ Oct 3, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
            "content": "TL;DR &#128064; . In a previous project, I made a Dataset by Scraping the videos details of a Youtube Channel using Selenium and Python. This time I&#39;ll be showing how to perform many tasks in order to process all the gathered information. The output of this project is a Clean and ready-to-analyse Dataset containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Also, here I leave you the Output Dataset from the previous Web Scraping Project, so you can compare them. . But first, let&#39;s learn a bit about the International Competition. Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion. Click here to learn more . Importing Libraries &#129520; . # Importing libraries import numpy as np import pandas as pd import re from datetime import datetime # check Pandas&#39; version pd.__version__ . &#39;1.1.5&#39; . Importing Dataset &#128451;&#65039; . # importing from url data_url = &#39;https://raw.githubusercontent.com/mrenrique/EDA-to-Youtube-Channel-Videos/main/redbulloficialgallos_videos_details_dec-27-2020.csv&#39; # reading dataset with pandas and asigning to a variable data = pd.read_csv(data_url) # show first three rows data.head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . Learning the Dataset&#39;s Properties &#128161; . Let&#39;s take a look at the datafame&#39;s properties for a better understanding to know what needs to be done. To do so, we can use the info() method which gives us the number of columns, columns names and their data types all together. . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 95 entries, 0 to 94 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 title 95 non-null object 1 views 95 non-null object 2 upload_date 95 non-null object 3 length 95 non-null object 4 likes 95 non-null object 5 dislikes 95 non-null object 6 url 95 non-null object dtypes: object(7) memory usage: 5.3+ KB . Now that we learn about the dataset in a general way, let&#39;s also learn in a detailed way by showing a random sample of the dataset to give us an idea of what kind of values we are dealing with. Let&#39;s start by showing a random sample of the dataset. . data.sample(frac=0.5).head() . title views upload_date length likes dislikes url . 19 TRUENO vs TITO MC - Octavos | Red Bull Interna... | 2,082,852 | Nov 30, 2019 | 5:51 | 43,267 | 3,996 | https://www.youtube.com/watch?v=KJbIAlUdmLw | . 71 JONY BELTRAN vs CHUTY - Octavos | Red Bull Int... | 13,138,438 | Nov 12, 2016 | 7:09 | 182,494 | 11,726 | https://www.youtube.com/watch?v=C2rXItCS8I0 | . 23 JAZE vs SNK - Octavos | Red Bull Internacional... | 1,407,134 | Nov 30, 2019 | 7:06 | 28,687 | 890 | https://www.youtube.com/watch?v=gkfOnJI4Byc | . 52 ARKANO vs. YENKY ONE - 3 y 4 Puesto: Final Int... | 2,093,488 | Dec 3, 2017 | 4:47 | 30,378 | 1,347 | https://www.youtube.com/watch?v=VOHgIr6dSZI | . 60 JONY BELTRAN vs. ARKANO - Cuartos: Final Inter... | 3,352,057 | Dec 3, 2017 | 4:41 | 37,794 | 1,164 | https://www.youtube.com/watch?v=wWtcdK7bd4Y | . Data Cleaning &#129532; and Transformation &#128298; . There are many tasks involved in Data Preprocessing which in turn are grouped into 4 main processes (Data Integration, Data Cleaning, Data Transformation and Data Reduction) but depending on the data and the scope of this project (Exploratory Data Analysis) we&#39;ll just need to perform some of them. let&#39;s start assuring the Data Quality for further Analysis. . Renaming Columns Names . Let&#39;s first show all Columns Names to check if they required changes. . data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;upload_date&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;], dtype=&#39;object&#39;) . As we see, almost all Columns Names are ok except for upload_date. Let&#39;s change it for year Since we only need the year of the date. . data.rename(columns={&#39;upload_date&#39;: &#39;year&#39;}, inplace=True) # Verify changes data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;year&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;], dtype=&#39;object&#39;) . data.dtypes . title object views object year object length object likes object dislikes object url object dtype: object . Deleting Columns not needed (First Attempt) . It&#39;s useful to remove some Columns that doesn&#39;t contributed to the Analysis Goal. In this case, url Column is not necesary. . data.drop(columns=[&#39;url&#39;], inplace=True) data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;year&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;], dtype=&#39;object&#39;) . Modifying values by Removing (Additional meaningless data), Adding or Formating them . Now in order to Set the proper Data Type to each Column we need to make sure that all Columns Values are clean. Let&#39;s see a few rows to know what kind of values the dataset has. . data.head() . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | . 3 RAPDER vs YARTZI - Cuartos | Red Bull Internac... | 47,082 | Dec 12, 2020 | 6:46 | 1,822 | 206 | . 4 EXODO LIRICAL vs BNET - Cuartos | Red Bull Int... | 685,109 | Dec 12, 2020 | 6:40 | 23,202 | 1,842 | . As you can see, there are some Undesired characters among the values of some Columns. So it&#39;s necesary to remove Unnecessary Characteres before doing any conversion task. Let&#39;s start cleaning the title Column to keep only the Names of Freestylers . . Important: Be careful, sometimes there are some characteres that seems similar like these ones - and ‚Äì but they are completely different and it can take you a while figure out why is not spliting as espected. I also had to add a conditional because the name of a participal has the - in it and it was spliting up incorrectly. . # Split by multiple different delimiters pattern = &#39;[-‚Äì|:]&#39; # data[&#39;title&#39;] = [re.split(pattern, i)[0].strip() if &#39;VALLES-T&#39; not in i else i for i in data[&#39;title&#39;]] data[&#39;title&#39;] = [re.split(pattern, i)[0].strip() if &#39;VALLES-T&#39; not in i else re.split(&#39; - &#39;, i)[0].strip() for i in data[&#39;title&#39;]] data[&#39;title&#39;] = [i.replace(&#39;.&#39;, &#39;&#39;).strip() for i in data[&#39;title&#39;]] # verify changes data[data[&#39;title&#39;].str.contains(&#39;VALLES&#39;)].head() . title views year length likes dislikes . 10 BNET vs VALLES-T | 1,350,908 | Dec 12, 2020 | 9:08 | 49,448 | 3,012 | . 18 BNET vs VALLES-T | 16,680,349 | Nov 30, 2019 | 17:12 | 282,481 | 32,957 | . 20 VALLES-T vs CHANG | 11,477,492 | Nov 30, 2019 | 6:43 | 161,561 | 2,969 | . 26 VALLES-T vs JOKKER | 3,221,089 | Nov 30, 2019 | 6:21 | 48,888 | 1,155 | . 31 VALLES-T vs ACZINO | 16,277,039 | Nov 30, 2019 | 13:46 | 279,388 | 9,027 | . Lets continue cleaning the Columns views, likes and dislikes, In this case, we&#39;ll remove the comma (,) from views, likes and dislikes Columns Values. Also, in the row 8 (and some others rows) there is the word Premiered before the date string. It needs to be removed. . # List of characters to remove chars_to_remove = [&#39; &#39;, &#39;,&#39;] # List of column names to clean cols_to_clean = [&#39;views&#39;, &#39;dislikes&#39;, &#39;likes&#39;] # Loop for each column for col in cols_to_clean: # Replace each character with an empty string for char in chars_to_remove: data[col] = data[col].astype(str).str.replace(char,&#39;&#39;) # verify changes data.head(3) . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | Dec 12, 2020 | 6:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | Dec 12, 2020 | 12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | Dec 12, 2020 | 10:06 | 18458 | 1146 | . As we said earlier, we only need the last part of the string for each upload_date Column Value. . data[&#39;year&#39;] = [re.split(&#39;,&#39;, i)[1].strip() for i in data[&#39;year&#39;]] # verify changes data[&#39;year&#39;].head() . 0 2020 1 2020 2 2020 3 2020 4 2020 Name: year, dtype: object . Data Type Convertion . (less memory usage) . Let&#39;s check what Data Types the Columns are . data.dtypes . title object views object year object length object likes object dislikes object dtype: object . Since we already saw the dataset have String, Datetime and Number values, this is not so specific, we need to set the right Data Type to all Columns. Let&#39;s first try an Automatic Data Type Conversion Method toy see if this will do the trick. . data.convert_dtypes().dtypes . title string views string year string length string likes string dislikes string dtype: object . Since we see the code above it&#39;s not quite effective, we&#39;ll need to convert them manually. Also, from the above code, we see that it&#39;s neccesary remove some characteres inside Columns Values, that&#39;s why the automatic method set all columns as a string. . data[&#39;title&#39;] = data[&#39;title&#39;].astype(str) # List of column names to convert to numberic data cols_to_modify_dtype = [&#39;views&#39;, &#39;dislikes&#39;, &#39;likes&#39;] for col in cols_to_modify_dtype: # Convert col to numeric data[col] = pd.to_numeric(data[col]) data[&#39;length&#39;] = pd.to_datetime(data[&#39;length&#39;], format=&#39;%M:%S&#39;).dt.time data[&#39;year&#39;] = pd.DatetimeIndex(data[&#39;year&#39;]).year # verify changes data.dtypes . title object views int64 year int64 length object likes int64 dislikes int64 dtype: object . Lets print once again a few rows of the dataset to see if changes were applied. . data.head() . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | 2020 | 00:06:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | 2020 | 00:12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | 2020 | 00:10:06 | 18458 | 1146 | . 3 RAPDER vs YARTZI | 47082 | 2020 | 00:06:46 | 1822 | 206 | . 4 EXODO LIRICAL vs BNET | 685109 | 2020 | 00:06:40 | 23202 | 1842 | . Dealing with Missing Values . First we verify if the dataset have Missing Values. . data.isnull().values.any() . False . Since there is not Missing Values, Let&#39;s move on to the next task. . Removing Duplicated Values . In order to identify if there are Duplicated Values, we&#39;ll use duplicated() method. . duplicateRowsDF = data[data.duplicated()] if duplicateRowsDF.empty == True: print(&#39;There arent Duplicated Values. Good to go!&#39;) else: print(&#39;Duplicate Rows except first occurrence based on all columns are :&#39;) print(duplicateRowsDF) . There arent Duplicated Values. Good to go! . Dealing with Inconsistencies Data . (Business Rule | Domain Expertice required) Modifying | Removing Erroneus Values . Because I&#39;m myself a fan of such Freestyle Competitions, I know that normally there are up to 16 matches every year. Let&#39;s verify that. . data[&#39;year&#39;].value_counts() . 2018 18 2020 17 2019 16 2017 16 2016 16 2015 12 Name: year, dtype: int64 . As we can see there are more than that in the year 2018 and 2020, Lets find out what&#39;s going on. . data[data[&#39;year&#39;] == 2018] . title views year length likes dislikes . 33 SWITCH vs BNET | 1637196 | 2018 | 00:07:19 | 33768 | 493 | . 34 WOS vs RAPDER Octavos | 1831544 | 2018 | 00:07:01 | 33093 | 5586 | . 35 BNET vs ARKANO | 3506340 | 2018 | 00:07:17 | 58063 | 1371 | . 36 VALLES T vs PEPE GRILLO | 10975462 | 2018 | 00:07:18 | 166863 | 2964 | . 37 NEON vs LETRA | 4750022 | 2018 | 00:07:36 | 77815 | 1101 | . 38 WOS vs LETRA | 3875917 | 2018 | 00:07:34 | 72927 | 5913 | . 39 VALLES T vs BNET | 3609190 | 2018 | 00:08:09 | 62295 | 4511 | . 40 WOS vs ACZINO | 39525308 | 2018 | 00:19:03 | 680254 | 73824 | . 41 VALLES T vs KDT | 1858540 | 2018 | 00:07:50 | 33888 | 877 | . 42 ACZINO vs JAZE | 4673494 | 2018 | 00:07:53 | 70535 | 5986 | . 43 BNET vs ACZINO | 6880359 | 2018 | 00:08:14 | 108931 | 8573 | . 44 YERIKO vs PEPE GRILLO | 263529 | 2018 | 00:07:14 | 5625 | 655 | . 45 RVS vs INDICO | 1488867 | 2018 | 00:10:48 | 31762 | 495 | . 46 INDICO vs ACZINO | 1578144 | 2018 | 00:07:28 | 21910 | 466 | . 47 VALLES T vs WOS | 6938112 | 2018 | 00:09:14 | 116773 | 25799 | . 48 DOZER vs ARKANO | 1231381 | 2018 | 00:08:07 | 27260 | 4257 | . 49 Perfil de Gallo | 16675 | 2018 | 00:00:52 | 863 | 30 | . 50 MARK GRIST vs GALLOS | 33867 | 2018 | 00:03:13 | 1442 | 44 | . Rows 49 and 50 are not part of the International Competition&#39; videos, so they need to be removed. Now, let&#39;s see the rows of 2020 year . data[data[&#39;year&#39;] == 2020] . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | 2020 | 00:06:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | 2020 | 00:12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | 2020 | 00:10:06 | 18458 | 1146 | . 3 RAPDER vs YARTZI | 47082 | 2020 | 00:06:46 | 1822 | 206 | . 4 EXODO LIRICAL vs BNET | 685109 | 2020 | 00:06:40 | 23202 | 1842 | . 5 SKONE vs ACERTIJO | 179664 | 2020 | 00:09:36 | 5341 | 4847 | . 6 ACZINO vs NAICEN | 158269 | 2020 | 00:06:51 | 5507 | 1713 | . 7 SKONE vs RAPDER | 1651540 | 2020 | 00:15:19 | 64965 | 3259 | . 8 ELEVN vs YARTZI | 56480 | 2020 | 00:06:30 | 2041 | 131 | . 9 RAPDER vs STICK | 122237 | 2020 | 00:06:21 | 4389 | 2710 | . 10 BNET vs VALLES-T | 1350908 | 2020 | 00:09:08 | 49448 | 3012 | . 11 EXODO LIRICAL vs MAC | 105707 | 2020 | 00:06:13 | 5319 | 254 | . 12 ACERTIJO vs MINOS | 57738 | 2020 | 00:06:24 | 2530 | 55 | . 13 SKONE vs TATA | 436674 | 2020 | 00:06:36 | 12055 | 17890 | . 14 NAICEN vs SNK | 66128 | 2020 | 00:06:44 | 3196 | 200 | . 15 ACZINO vs SHIELD MASTER | 154369 | 2020 | 00:06:51 | 5439 | 471 | . 16 BLON vs NEW ERA vs YOIKER | 932161 | 2020 | 00:25:18 | 49375 | 859 | . The same, Even though the row 16 is a international Competition Video (info), this match was done to have a reserve competitor just in case any of the 16 couldn&#39;t make it. But it didn&#39;t occur. Now let&#39;s remove all rows are not part of the Oficial Matches&#39; Videos. . data = data.drop([16, 49 , 50]) . Setting &amp; Modifying the Index Column . Bencause it was necessary to remove some rows (16,49 and 50), the index was changed. Let fix that. Also, I&#39;ll asign a name to the Index Column. . data.reset_index(inplace = True, drop=True) . data.loc[48:50,:] . title views year length likes dislikes . 48 G vs EL TANQUE | 253069 | 2017 | 00:05:08 | 3640 | 424 | . 49 ARKANO vs YENKY ONE | 2093488 | 2017 | 00:04:47 | 30378 | 1347 | . 50 WOS vs ACZINO | 23008624 | 2017 | 00:07:51 | 261785 | 13990 | . Exporting the Clean Dataset &#128190; . Now that we&#39;re assure the dataset is clean and contain only the right values. Let&#39;s export it to move on to Exporing and Analizing the dataset. . data.to_csv(&#39;clean_data.csv&#39;) . Or if you prefer, you can download it to your Computer. . data.to_csv(&#39;clean_data.csv&#39;) from google.colab import files files.download(&#39;clean_data.csv&#39;) . You&#39;re Awesome, you just reached the end of this post. If you have any questions just drop me a message. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I&#39;m sure you&#39;ll find something interesting üí°. Share this post with your friends/colleagues on (Facebook, Linkedint or Twitter) or if you are in a good mood, buy me a cup of coffee ‚òï. Nos vemos üèÉüí® .",
            "url": "https://mrenrique.github.io/portfolio/data%20preprocessing/python/data%20science/pandas/numpy/2021/01/10/data-preprocessing-with-pandas-numpy.html",
            "relUrl": "/data%20preprocessing/python/data%20science/pandas/numpy/2021/01/10/data-preprocessing-with-pandas-numpy.html",
            "date": " ‚Ä¢ Jan 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
            "content": "TL;DR &#128064; . This project is to perform most common tasks of Web Scraping by using Selenium as a Scraper Tool and Python for coding. The output will be a CSV file containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Here you can take a peek or download the csv file which is the result of this project. (Also added at the bottom of this notebook) . FYI: Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion Click here to learn more . Her I leave you a screenshot of the Youtube Channel used for this project: . . Satisfying the requirements . As always, let&#39;s first install libraries we&#39;ll be using thought the project These ones are Chromium(browser), Selenium (scraper tool), and tqdm (progress bar). . Installing Libraries &#10004;&#65039; . # install chromium, selenium and tqdm !apt update !apt install chromium-chromedriver !cp /usr/lib/chromium-browser/chromedriver /usr/bin !pip install selenium !pip install tqdm print(&#39;Library installation Done!&#39;) . Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB] Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B] Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 InRelease Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 Release Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Release Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB] Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [41.5 kB] Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,700 kB] Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB] Fetched 2,884 kB in 4s (790 kB/s) Reading package lists... Done Building dependency tree Reading state information... Done 17 packages can be upgraded. Run &#39;apt list --upgradable&#39; to see them. Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra Suggested packages: webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin The following NEW packages will be installed: chromium-browser chromium-browser-l10n chromium-chromedriver chromium-codecs-ffmpeg-extra 0 upgraded, 4 newly installed, 0 to remove and 17 not upgraded. Need to get 81.0 MB of archives. After this operation, 273 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB] Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB] Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB] Fetched 81.0 MB in 5s (15.5 MB/s) Selecting previously unselected package chromium-codecs-ffmpeg-extra. (Reading database ... 145480 files and directories currently installed.) Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-browser. Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-browser-l10n. Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ... Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-chromedriver. Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ... update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ... Processing triggers for hicolor-icon-theme (0.17-2) ... Processing triggers for mime-support (3.60ubuntu1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... cp: &#39;/usr/lib/chromium-browser/chromedriver&#39; and &#39;/usr/bin/chromedriver&#39; are the same file Collecting selenium Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 911kB 8.4MB/s Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3) Installing collected packages: selenium Successfully installed selenium-3.141.0 Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1) Library installation Done! . . Importing Libraries &#129520; . Once Installed, We&#39;ll procced to import them. . # set options to be headless from selenium import webdriver #the followings are to avoid NoSuchElementException by using WebDriverWait - to wait until an element appears in the DOM from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC # add random pause seconds to avoid getting blocked import time, random # to use a progress bar for visual feedback from tqdm import tqdm # to get the current date from datetime import date # to save Dataframe into a CSV file format import pandas as pd import numpy as np # Upload or download files from google.colab import files print(&#39;All Libraries imported!&#39;) . . All Libraries imported! . Phase 01: Accessing the Web Page &#127760; . Opening the Browser and Visiting the Target Web Page . # Setting options for the web browser chrome_options = webdriver.ChromeOptions() chrome_options.add_argument(&#39;-headless&#39;) chrome_options.add_argument(&#39;-no-sandbox&#39;) chrome_options.add_argument(&#39;-disable-dev-shm-usage&#39;) # Open browser, go to a website, and get results browser = webdriver.Chrome(&#39;chromedriver&#39;,options=chrome_options) browser.execute_script(&quot;return navigator.userAgent;&quot;) print(browser.execute_script(&quot;return navigator.userAgent;&quot;)) channel_url = &#39;https://www.youtube.com/c/RedbullOficialGallos/videos&#39; # Open website browser.get(channel_url) # Print page title print(browser.title) . Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/87.0.4280.66 Safari/537.36 Red Bull Batalla De Los Gallos - YouTube . Reaching the bottom of this Dynamically Loaded Page . Since this Page&#39;s content is dinamically loaded by scrolling down, we use a function to dinamically change the scrollHeight. . def scroll_to_the_page_bottom(browser): height = browser.execute_script(&quot;return document.documentElement.scrollHeight&quot;) lastheight = 0 while True: if lastheight == height: break lastheight = height browser.execute_script(&quot;window.scrollTo(0, &quot; + str(height) + &quot;);&quot;) # Pause 2 seconds per iteration time.sleep(2) height = browser.execute_script(&quot;return document.documentElement.scrollHeight&quot;) print(&#39;The scroll down reached the bottom of the page, all content loaded!&#39;) scroll_to_the_page_bottom(browser) . The scroll down reached the bottom of the page, all content loaded! . Phase 02: Scraping the data &#9935;&#65039; . Getting the links of all videos . video_anchors = browser.find_elements_by_css_selector(&#39;#video-title&#39;) print(f&#39;This Channel has {len(video_anchors)} videos published&#39;) . This Channel has 3226 videos published . For this project, we&#39;re gonna gather all the videos link that contains the words: . internacional | vs | . To do so, we&#39;ll use a list comprehension along with all(). . We&#39;re using all() instead of any because we want to filter having all elements present inside each text item. Think about it as the and operator.the any() method then would be like any, because any text item that match at least one of the matches would be inserted in the list called video_links. . # initializing list of keywords to filter (16 videos only should be) matchers = [x.lower() for x in [&#39;Internacional&#39;, &#39;vs&#39;]] video_links = [link.get_attribute(&#39;href&#39;) for link in tqdm(video_anchors, position=0) if all(match in link.text.lower() for match in matchers)] print(len(video_links)) #Show the first link video_links[0] . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3226/3226 [02:30&lt;00:00, 21.44it/s] . 95 . . &#39;https://www.youtube.com/watch?v=Fwda4AWZ6V4&#39; . Getting all details for each video . Now, we are going to retrieve the details of all videos we are interested in such us title, views, upload date, lenght of video,likes and dislikes. We&#39;ll use a for loop to iterate over the video_links variable which contains all videos&#39; urls and per each url we extract the data and save them in variables. . Once saved the collection or variables are stored in a Dictionary called data. Finally each dictionary are saved in the variable video_details which basically is a list of dictionaries containing all details per each video scraped. Let&#39;s jump in the code to better understanding. . video_details = [] delay = 10 for link in tqdm(video_links, desc=&#39;Getting all details for each video&#39;, position=0, leave=True): try: browser.get(link) except: continue # Pause 3 seconds to load content time.sleep(3) # Get element after explicitly waiting for up to 10 seconds title = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.title&#39;))).text views = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.view-count&#39;))).text.split(&#39; n&#39;)[0].split()[0] upload_date = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;#date &gt; yt-formatted-string&#39;))).text length = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.ytp-time-duration&#39;))).text likes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , &#39;#top-level-buttons #text&#39;)))[0].get_attribute(&#39;aria-label&#39;).split()[0] dislikes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , &#39;#top-level-buttons #text&#39;)))[1].get_attribute(&#39;aria-label&#39;).split()[0] url = link # inserting all data in the list. We&#39;ll also use aternary expression/operator to save a value depending on a condition data = { &#39;title&#39;: title, &#39;views&#39;: views, &#39;upload_date&#39;: upload_date, &#39;length&#39;: length, &#39;likes&#39;: likes, &#39;dislikes&#39;: dislikes, &#39;url&#39;: url } video_details.append(data) # Pause 3 seconds per iteration time.sleep(3) # Close the browser once the for loop is done browser.quit() print(f&#39;All details of {len(video_links)} videos successfully retrieved&#39;) . Getting all details for each video: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [11:53&lt;00:00, 7.51s/it] . All details of 95 videos successfully retrieved . . Excellent, we just got all videos details and insert them into a list called video_details for convinence. . To verify the details per each video were saved correctly let&#39;s print the first element whitin the list. . video_details[0] . {&#39;dislikes&#39;: &#39;270&#39;, &#39;length&#39;: &#39;6:16&#39;, &#39;likes&#39;: &#39;14,040&#39;, &#39;title&#39;: &#39;ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | Red Bull Internacional 2020&#39;, &#39;upload_date&#39;: &#39;Dec 12, 2020&#39;, &#39;url&#39;: &#39;https://www.youtube.com/watch?v=Fwda4AWZ6V4&#39;, &#39;views&#39;: &#39;577,503&#39;} . Phase 03: Saving the Gathered Data &#128190; . Saving data to a CSV file . To dynamically name our output csv file, we&#39;ll use from datetime import date which is already imported in the Importing Libraries Section. Let&#39;s first get the current date and the Youtube Channel&#39;s Name from the url we provided. . today = date.today() # Month abbreviation, day and year todays_date = today.strftime(&quot;%b-%d-%Y&quot;) print(f&#39;Fecha de hoy: {todays_date}&#39;) channel_name = channel_url.split(&#39;/&#39;)[4] print(channel_name) . . Fecha de hoy: Dec-27-2020 RedbullOficialGallos . Now, let&#39;s put all variables together to name the file. . # Programatically naming csv file csv_file_name = f&#39;{channel_name}_videos_details_{todays_date}.csv&#39;.lower() print(csv_file_name) # Assign columns names field_names = [&#39;title&#39;, &#39;views&#39;, &#39;upload_date&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;] . . redbulloficialgallos_videos_details_dec-27-2020.csv . We&#39;re almost done, with the csv_file_name and field_names variables, let&#39;s turn video_details into a Dataframe which can be used later for any analysis. We&#39;ll need to install Pandas and Numpy to do so. These libraries were already imported in the Importing Libraries Section. . # Create DataFrame df = pd.DataFrame(video_details, columns=field_names) # Show first 3 rows to verify the dataframe creation df.head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . # Save Dataframe into a CSV file format df.to_csv(csv_file_name, index=False) # Read the file and print the first 3 rows to verify its creation pd.read_csv(csv_file_name).head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . Yay! You reach the end of this article. By now you know how retrieve all videos details from a Youtube Channel. As earlier mentioned, the scraped data should be in the generated csv file. If you worked on it in Jupyter Notebook or your Favorite Code Editor, you can find it in the same folder where you ran your¬†.pynb file. But, if you worked on Google Colab (like me), you need to use the following code to download it: from google.colab import files. This library was already imported in the Importing Libraries Section . # Download the file that contains the scraped table files.download(csv_file_name) print(&#39;In a moment the option &quot;Save As&quot; will appear to download the file...&#39;) . In a moment the option &#34;Save As&#34; will appear to download the file... . Takeaways . Since Youtube is a loading content Page, I&#39;ve decided to use Selenium as a tool to scrape | When scraping the video_lenght of a video, For some reason some of them return a None value, so we need to use its text version from the arial-label of the same element | I&#39;ve decided to use Pandas instead of the CSV library to create and save the Dataframe into a CSV file because it&#39;s easier to use. | The elements were accesed using its css selector because is faster and easier to read | This project is to show off skills of Web Scraping using Selenium. For the next tutorial, we&#39;ll do the same but using the Youtube API | Since this project&#39;s scope is just to gather all data needed in a machine readable format (CSV). What remains to be done is Data Preprocessing and Exploratory Data Analysis | . Here I leave you the csv file we&#39;ve just scraped from Youtube. . References . This is where I got inspiration from . How to Extract &amp; Analyze YouTube Data using YouTube API? . Using Selenium wthinin Google Colab . Scroll to end of page in dynamically loading webpage. Answered by: user53558 . Saving a Pandas Dataframe as a CSV . Scroll to end of page in dynamically loading webpage . Asign variables to dictionary based on value . WebDriverWait on finding element by CSS Selector . Use of if else inside a dict to set a value to key using Python . How to get back to the for loop after exception handling . tqdm printing to newline .",
            "url": "https://mrenrique.github.io/portfolio/web%20scraping/python/data%20science/pandas/selenium/2020/12/27/web-scraping-youtube-channel-selenium.html",
            "relUrl": "/web%20scraping/python/data%20science/pandas/selenium/2020/12/27/web-scraping-youtube-channel-selenium.html",
            "date": " ‚Ä¢ Dec 27, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
            "content": "¬øAlguna vez te topaste con alguna tabla de Wikipedia que justo necesitabas para tu investigaci√≥n o proyecto de an√°lisis? Como la tabla de medallas de los Juegos Panamericanos, Las Elecciones Presidenciales o datos de las Exportaciones de un Pais, ¬øSi? Bueno, Aqu√≠ aprender√°s como obtener datos de cualquier tabla. . Esta es la tabla que buscamos obtener que contiene informaci√≥n del Censo de la Provincia de Trujillo del a√±o 2017). Para hacer Web Scraping a esta p√°gina, utilizaremos las bibliotecas Requests y Pandas de Python. . . Al finalizar este Tutorial, tendr√°s como resultado una tabla lista para su an√°lisis. Lo podr√°s guardar en tu directorio de trabajo o descargar en tu computadora como archivo CSV (o el formato que gustes). Espero que te resulte √∫til; sin m√°s que a√±adir... &quot;Happy Coding!&quot; . Fase 1: Conecci&#243;n y obtenci&#243;n de codigo fuente . Comenzaremos importando la biblioteca requests para realizar la petici√≥n HTTP que nos devolver√° el c√≥digo fuente de la p√°gina web y pandas, para usar su m√©todo .read_html() que nos ayudar√° a extraer todas las tablas HTML. Usaremos este m√©todo y no la biblioteca Beautiful Soup porque de esta forma es mucho m√°s f√°cil y funciona bien en cualquier p√°gina web que contenga tablas HTML debido a su estructura simple de leer. . Para tu conocimiento:Usando .read_html() no solo nos permite extraer tablas HTML desde un enlace (lo que usaremos hoy), sino tambi√©n desde un archivo html o una cadena de caracteres (string) que contenga HTML. . import requests # Manipular c√≥digo y guardar datos tabulares en archivo CSV import pandas as pd # url de la p√°gina web a ¬´escrapear¬ª url = &#39;https://es.wikipedia.org/wiki/Provincia_de_Trujillo_(Per%C3%BA)&#39; # pasar &quot;User-agent&quot; para simular interacci√≥n con la p√°gina usando Navegador web headers = {&quot;User-agent&quot;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36&#39;} respuesta = requests.get(url, headers=headers) # El c√≥digo de respuesta &lt;200&gt; indicar√° que todo sali√≥ bien print(respuesta) . &lt;Response [200]&gt; . Aparte de la respuesta positiva que nos devolvi√≥ la petici√≥n, seguro te diste cuenta que pasamos el par√°metro headers al m√©todo requests.get(), esto sirve para evitar que el servidor de la p√°gina web nos bloqueen el acceso. Aunque con Wikipedia no tenemos ese problema, otras p√°ginas son restrictivas cuando se trata de bots tratando de ¬´escrapear¬ª sus datos. Siguiento esta buena pr√°ctica, nos ahorarremos dolores de cabeza luego. . Por √∫ltimo, guardamos en una variable todas las tablas encontradas en el objecto HTML. . all_tables = pd.read_html(respuesta.content, encoding = &#39;utf8&#39;) . Fase 2: An&#225;lisis de estructura HTML y extracci&#243;n de datos . Ahora veamos cuantas tablas hay en la p√°gina web . print(f&#39;Total de tablas encontradas: {len(all_tables)}&#39;) . Total de tablas encontradas: 6 . Debido a que hay varias tablas, usaremos el par√°metro match y le pasaremos una palabra o frase que solo se encuentre en la tabla que nos interesa. . matched_table = pd.read_html(respuesta.text, match=&#39;Fuente: Censos Nacionales 2017&#39;) # imprime numero de tablas que coinciden con parametro match print(f&#39;Total de tablas encontradas: {len(matched_table)}&#39;) . Total de tablas encontradas: 1 . Guardamos nuestra tabla de inter√©s en una variable. . censo_trujillo = matched_table[0] # Verificamos si es la tabla que buscamos censo_trujillo.tail(5) . UBIGEO Distrito Hogares Viviendas Poblaci√≥n . 8 130109 | Salaverry | 5599 | 5244 | 18¬†944 | . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | V√≠ctor Larco Herrera | 19¬†543 | 18¬†461 | 68¬†506 | . 11 NaN | TOTAL | 273¬†619 | 250¬†835 | 970¬†016 | . 12 Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | . ¬°Bien! Es la tabla que buscamos exportar, pero vemos que las 2 √∫ltimas filas no son necesarias, por lo que pasamos a eliminarlas. . censo_trujillo.drop(censo_trujillo.tail(2).index, inplace=True) # Verificar si se eliminaron los registros no deseados censo_trujillo.tail(2) . UBIGEO Distrito Hogares Viviendas Poblaci√≥n . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | V√≠ctor Larco Herrera | 19¬†543 | 18¬†461 | 68¬†506 | . Ahora asignaremos el UBIGEO como √≠ndice de la tabla. . censo_trujillo.set_index(&#39;UBIGEO&#39;, inplace = True) # Verificamos el cambio de √≠ndice censo_trujillo.head(2) . Distrito Hogares Viviendas Poblaci√≥n . UBIGEO . 130101 Trujillo | 87¬†963 | 82¬†236 | 314¬†939 | . 130102 El Porvenir | 57¬†878 | 50¬†805 | 190¬†461 | . Tambi√©n, vemos que en las columnas Hogares,Viviendas y Poblaci√≥n que contienen n√∫meros, hay un espacio en medio de los n√∫meros que le da Wikipedia como formato para mejorar su visualizaci√≥n. Sin embargo, necesitamo que nuestros datos num√©ricos est√©n limpios sin ning√∫n simbolo o espacios para poder realizar operaciones. . Removamos ese espacio que no aporta nuestros datos. Para ello usaremos la biblioteca normalize. . from unicodedata import normalize . Creamos una funcion y lo correremos a toda nuestra tabla para quitar los molestos espacios en blanco. . def remove_whitespace(x): &quot;&quot;&quot;Funcion para normalizar datos con Unicode para luego quitar los espacios usando .replace(). Argumentos de entrada: Nombre de columna o lista con nombres de columnas. Retorna: columna o columnas sin espacios en blanco &quot;&quot;&quot; if isinstance(x, str): return normalize(&#39;NFKC&#39;, x).replace(&#39; &#39;, &#39;&#39;) else: return x . Aplicamos la funcion para quitar espacios en blanco a toda las columnas con datos num√©ricos. . numeric_cols = [&#39;Hogares&#39;,&#39;Viviendas&#39;,&#39;Poblaci√≥n&#39;] # Aplicar funci√≥n remove_whitespace a columnas en variable y las reemplazamos en tabla censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].applymap(remove_whitespace) # Verificamos si se quitaron los espacios en blanco censo_trujillo.head() . Distrito Hogares Viviendas Poblaci√≥n . UBIGEO . 130101 Trujillo | 87963 | 82236 | 314939 | . 130102 El Porvenir | 57878 | 50805 | 190461 | . 130103 Florencia De Mora | 7777 | 8635 | 37262 | . 130104 Huanchaco | 20206 | 16534 | 68409 | . 130105 La Esperanza | 49773 | 47896 | 189206 | . Ahora veamos los tipos de datos. . censo_trujillo.dtypes . Distrito object Hogares object Viviendas object Poblaci√≥n object dtype: object . Como vemos, todos las columnas son de tipo de dato Objeto (lo que Pandas considera como una cadena de caracteres o String). Como Object es muy amplio, necesitamos definir el tipo de dato correcto a cada columna para que luego se realizar operaciones con los datos. . Aqu√≠ va la primera opci√≥n para hacerlo, reusando la variable numeric_cols usada l√≠neas arriba. . censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].apply(pd.to_numeric) # Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado censo_trujillo.dtypes . Distrito object Hogares int64 Viviendas int64 Poblaci√≥n int64 dtype: object . Como ves, nuestras columnas con n√∫meros ya tienen el formato correcto, pero la columna Distrito a√∫n se mantiene como Object. Aunque no habr√≠a mayor inconveniente, es mejor especificar que datos contiene cada columna. . Aqu√≠ va la seguida opci√≥n para cambiar el tipo de dato a m√∫ltiples columnas. . convert_dict = { &#39;Distrito&#39;: &#39;string&#39;, &#39;Hogares&#39;: &#39;int&#39;, &#39;Viviendas&#39;: &#39;int&#39;, &#39;Poblaci√≥n&#39;: &#39;int&#39; } censo_trujillo = censo_trujillo.astype(convert_dict) # Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado censo_trujillo.dtypes . Distrito string Hogares int64 Viviendas int64 Poblaci√≥n int64 dtype: object . Fase 3: Guardado del Conjunto de Datos . Por fin, una ves los datos est√°n limpios y con el tipo de dato correcto, vamos a guardarlos en formato CSV para usarlos luego. . censo_trujillo.to_csv(&#39;censo_provincia_trujillo_2017.csv&#39;) # Leamos el archivo para verificar su creacion pd.read_csv(&#39;censo_provincia_trujillo_2017.csv&#39;).head(3) . UBIGEO Distrito Hogares Viviendas Poblaci√≥n . 0 130101 | Trujillo | 87963 | 82236 | 314939 | . 1 130102 | El Porvenir | 57878 | 50805 | 190461 | . 2 130103 | Florencia De Mora | 7777 | 8635 | 37262 | . Si lo trabajaste en Jupyter Notebook o tu Editor de C√≥digo Favorito, debe estar ubicado en la misma carpeta donde corriste tu archivo .pynb. Pero, si lo trabajaste en Google Colab (como yo), puedes usar la biblioteca files para descargar el archivo en la computadora donde est√©s trabajando. . from google.colab import files # Descarga archivo con datos de tabla files.download(&quot;censo_provincia_trujillo_2017.csv&quot;) print(&#39;Listo, en un momento saldr√° la opci√≥n &quot;Guardar Como&quot; para descargar el archivo...&#39;) . Listo, en un momento saldr√° la opci√≥n &#34;Guardar Como&#34; para descargar el archivo... . ¬°Genial! Ya tenemos los datos de una tabla de Wikipedia lista para su uso. Aunque la tabla extraida es peque√±a en dimensi√≥n, esta forma de trabajo puedes aplicarla para extraer cualquier tabla que encuentres interesante, ya sea de Wikipedia o de cualquier otra p√°gina web que contenga tablas HTML. . Resumiendo lo realizado . Le√≠mos las tablas HTML de una p√°gina de Wikipedia | Removimos los espacios en formato Unicode que imped√≠an la conversi√≥n al tipo de dato correcto | Convertimos el tipo de dato de todas las columnas al correcto | Guardamos la tabla extra√≠da como formato csv para su posterior utilizaci√≥n | Finalmente, descargamos el archivo csv en la computadora de trabajo | . ¬°Espera! Una cosa m√°s, como est√°mos en el mes de diciembre, te regalo este pensamiento: . Si lo lees y lo entiendes est√° genial, aunque eso no te asegura haberlo aprendido. Para dominarlo necesitas practicarlo. . Por eso, te sugiero que guardes este art√≠culo en tus Favoritos para que lo practiques luego. O si quieres practicarlo ya mismo, aqu√≠ te dejo el art√≠culo en Google Colab, donde podr√°s abrir y correr el c√≥digo sin necesidad de instalar nada, solo tu navegador web y tus ganas de aprender. . Si te gust√≥, puedes ver mis otras publicaciones, seguro te ser√°n de utilidad. Si gustas apoyarme, comparte este art√≠culo en tus Redes Sociales (Facebook, Linkedint, Twitter) o si est√°s de buen √°nimo, inv√≠tame una taza de caf√© ‚òï. Nos vemos üèÉüí® .",
            "url": "https://mrenrique.github.io/portfolio/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "relUrl": "/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "date": " ‚Ä¢ Dec 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Data Analysis in a Nutshell",
            "content": "What is Data Analysis . A process of inspecting, cleansing, transforming and modeling data with the goal of discovering useful information, informing conclusion and supporting decision-making. Source: Wikipedia . Uses of EDA: . To know the structure and distribution of data | To find relationship between Features | To find relationship between Features and the Target Variable | To find errors, anomalies, outliers | To refine Hipothesis or generate new questions on dataset | . Data Analysis Tools . Programming Languages: Open Source, Free, Extremely Powerful, Steep learning curve . Python | R | Julia | . Auto-managed closed tools: Closed Source, Expensive, Limited, Easy to learn . Power BI | Tableau | Qlik | . The Data Analysis Process . Data Extraction . SQL | Scrapping | File Formats CSV | JSON | XML | . | Consulting APIs | Buying Data | Distributed Databases | . Data Cleaning . Missing values and empty data | Data imputation | Incorrect types | Incorrect or invalid values | Outliers and non relevant data | Statistical sanitization | . Data Wrangling . Hierarchical Data | Handling categorical data | Reshaping and transforming structures | Indexing data for quick access | Merging, combining and joining data | . Analysis . Exploration | Building statistical models | Visualization and representations | Correlation vs Causation analysis | Hypothesis testing | Statistical analysis | Reporting | . Action . Building Machine Learning Models | Feature Engineering | Moving ML into production | Building ETL pipelines | Live dashboard and reporting | Decision making and real-life tests | . https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html . Proceso de organizar, resumir y visualizar un conjunto de datos para extraer informaci√≥n que aporte al logro de objetivos . why using Python and Pandas? . The Pandas library is the key library for Data Science and Analytics and a good place to start for beginners. Often called the &quot;Excel &amp; SQL of Python, on steroids&quot; because of the powerful tools Pandas gives you for editing two-dimensional data tables in Python and manipulating large datasets with ease. . Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with Matplotlib and Seaborn, Pandas provides a wide range of opportunities for visual analysis of tabular data. . The main data structures in Pandas are implemented with Series and DataFrame classes. DataFrames are great for representing real data: rows correspond to instances (examples, observations, etc.), and columns correspond to features of these instances. . Main Keywords . Dataframe: is a main Object in Pandas, It&#39;s used to represent data in rows and columns (Tabular Data) | Pandas: This library needs no introduction as it became the de facto tool for Data Analysis in Python. The name pandas is derived from the term ‚Äúpanel data‚Äù, an econometrics term for datasets that include observations over multiple time periods for the same individuals. | .",
            "url": "https://mrenrique.github.io/portfolio/data%20analysis/2020/02/20/test.html",
            "relUrl": "/data%20analysis/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello World (üëãüåé) Welcome to my thoughts and projects on data science I‚Äôve been documenting through blogging ‚úçÔ∏è. My learning path is mainly base on üìö books and MOOC‚Äôs üë®‚Äçüíª from Coursera, Edx and related resources i‚Äôve collected myself. Let‚Äôs learn together. üí™üíØ .",
          "url": "https://mrenrique.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://mrenrique.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}